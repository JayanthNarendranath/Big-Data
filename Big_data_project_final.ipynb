{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0b802b-175b-4cf2-90ea-7c7073bc49cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8e8876-b97b-41d2-b1e3-e3474c1bce62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import NGram, VectorAssembler, StopWordsRemover, HashingTF, IDF, Tokenizer, StringIndexer, ChiSqSelector, VectorAssembler, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41a011d-0fa2-41ed-8a8c-5a034c381d00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mount_s3_bucket(access_key, secret_key, bucket_name, mount_folder):\n",
    "  ACCESS_KEY_ID = access_key\n",
    "  SECRET_ACCESS_KEY = secret_key\n",
    "  ENCODED_SECRET_KEY = SECRET_ACCESS_KEY.replace(\"/\", \"%2F\")\n",
    "\n",
    "  print (\"Mounting\", bucket_name)\n",
    "\n",
    "  try:\n",
    "    # Unmount the data in case it was already mounted.\n",
    "    dbutils.fs.unmount(\"/mnt/%s\" % mount_folder)\n",
    "    \n",
    "  except:\n",
    "    # If it fails to unmount it most likely wasn't mounted in the first place\n",
    "    print (\"Directory not unmounted: \", mount_folder)\n",
    "    \n",
    "  finally:\n",
    "    # Lastly, mount our bucket.\n",
    "    dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY_ID, ENCODED_SECRET_KEY, bucket_name), \"/mnt/%s\" % mount_folder)\n",
    "    #dbutils.fs.mount(\"s3a://\"+ ACCESS_KEY_ID + \":\" + ENCODED_SECRET_KEY + \"@\" + bucket_name, mount_folder)\n",
    "    print (\"The bucket\", bucket_name, \"was mounted to\", mount_folder, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89cc1281-2938-4e8f-adad-36057e8de046",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set AWS programmatic access credentials\n",
    "ACCESS_KEY = \"AKIA6NYFE23R7RKKBOHM\"\n",
    "SECRET_ACCESS_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acad05b2-b881-4a61-b24f-6dfcd75f9f5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, \"weclouddata/twitter\", \"big_data_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0182686-8c46-4f48-a300-62581f0588c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /mnt/big_data_project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0bdedd0-a52b-44d4-b5ca-d8dcd5c68a6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /mnt/big_data_project/StudentLoanRelief/2022/12/13/02/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9557171-4981-436f-a529-f1aa6fb74b04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = '/mnt/big_data_project/StudentLoanRelief/*/*/*/*/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1d440c-72a8-4261-8a3a-1662b6ecfdde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Big_data_project') \\\n",
    "        .getOrCreate()\n",
    "print('Session created')\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61471389-14f8-4486-9e3b-0f671f22d1e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tweetsSchema = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('user_name', StringType(), True),\n",
    "    StructField('screen_name', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('followers_count', IntegerType(), True),\n",
    "    StructField('location', StringType(), True),\n",
    "    StructField('geo', StringType(), True),\n",
    "    StructField('created_at', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a189ec-2f99-4d2d-b9b4-b8fc598c754b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option('header',False).option('delimiter', '\\t').schema(tweetsSchema).csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ef07e06-f67f-4daa-b812-d9fa195edafe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cache the dataframe for faster iteration\n",
    "df.cache() \n",
    "\n",
    "# run the count action to materialize the cache\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52ea9d6-5b3e-41ed-910e-34cc2abb6fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9221a19c-6e8e-4f05-bb88-e1de01f07240",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Text Cleaning Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb22b13c-3486-415c-8726-2ddf650abe75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df.withColumn('text', F.regexp_replace('text', r\"http\\S+\", \"\")) \\\n",
    "                    .withColumn('text', F.regexp_replace('text', r\"[^a-zA-z]\", \" \")) \\\n",
    "                    .withColumn('text', F.regexp_replace('text', r\"\\s+\", \" \")) \\\n",
    "                    .withColumn('text', F.lower('text')) \\\n",
    "                    .withColumn('text', F.trim('text')) \n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "572160a3-d062-48f2-804b-9fd0825b0abb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Creating a Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1359a0-aeb6-4c73-863a-2023fe87e754",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    blob=TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    if sentiment > 0:\n",
    "        return 'positive'\n",
    "    if sentiment < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d5fa0e-e480-4b2d-9b9f-d4305a1a6cfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sentiment_udf = udf(get_sentiment,StringType())\n",
    "df_clean = df_clean.withColumn('sentiment',sentiment_udf(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718ffa6f-83e0-4625-a351-aa943f49de43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3910066a-e733-469a-8075-992d6ce484f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mount_s3_bucket(ACCESS_KEY, SECRET_ACCESS_KEY, \"wcd-jayanth/\", \"big_data_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4159f5c5-896d-45d8-b351-3856650bd3a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /mnt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0dd525d-7658-400d-8e25-93ed86de4666",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clean.write.option('header', False).option('delimiter','\\t').mode('overwrite').csv('/mnt/big_data_project/raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20bab050-5aa5-4d74-aeba-a03cc38e9329",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Feature Transformer:Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c1852c-37e6-4ed7-841b-cf261c4abdbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='tokens')\n",
    "tweets_tokenized = tokenizer.transform(df_clean)\n",
    "\n",
    "display(tweets_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e15d7f4-f1c5-4019-9b1c-242073d0db78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Feature Transformer:Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f8b103-bf19-41f4-b1ed-54eb68a7b5c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#now remove stopwords from the review(list of words)    \n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "tweets_stopword = stopword_remover.transform(tweets_tokenized)\n",
    "\n",
    "display(tweets_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba718363-fe5e-425b-9b03-79075c3cba60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Feature Transformer: CountVectorizer (TF - Term Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63c4d78-a84f-443f-8f3d-ca0fc11e4273",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "cv_model = cv.fit(tweets_stopword)\n",
    "tweets_cv = cv_model.transform(tweets_stopword)\n",
    "\n",
    "display(tweets_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70f86fcf-c3e5-4d4c-b417-5405d598029b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5. Feature Transformer: TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82563c97-6437-43b0-a8a6-27fc9924eb00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_model = idf.fit(tweets_cv)\n",
    "tweets_idf = idf_model.transform(tweets_cv)\n",
    "\n",
    "display(tweets_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "174047e8-9f6f-44f8-b0e1-b90443056ebc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194159c3-f7f0-42c7-bc6d-6dea72d44d94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "label_encoder = StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "le_model = label_encoder.fit(tweets_idf)\n",
    "tweets_label = le_model.transform(tweets_idf)\n",
    "\n",
    "display(tweets_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d4d615b-5d11-4c1d-8f1a-c6a037fd31d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 7. Model Training: Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1caa0443-6bc8-407f-b7c5-eebd7933d520",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "\n",
    "lr_model = lr.fit(tweets_label)\n",
    "\n",
    "predictions = lr_model.transform(tweets_label)\n",
    "\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "777effd5-1512-41b8-8682-b7e288424bbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd25f04b-57b5-42b3-9e18-fa7db96df436",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a78436-ef3a-4e5c-bbb1-50b2e456ecca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(predictions.write.mode(\"overwrite\")\n",
    " .parquet('/mnt/big_data_project/raw_data.csv/big_data_project_predictions.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8cf7558-3ba2-46f7-aff6-72971bc99fac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0faaf86a-d5d9-4f6b-95a8-b9590ae66649",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 9. Putting a pipeline together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b36446c-e003-49d4-a374-107fd9c068e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, VectorAssembler, StopWordsRemover, HashingTF, IDF, Tokenizer, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Use 90% cases for training, 10% cases for testing\n",
    "train, test = df_clean.randomSplit([0.9, 0.1], seed=20200819)\n",
    "\n",
    "# Create transformers for the ML pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"1gram_idf\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "assembler = VectorAssembler(inputCols=[\"1gram_idf\"], outputCol=\"features\")\n",
    "label_encoder= StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, stopword_remover, cv, idf, assembler, label_encoder, lr])\n",
    "\n",
    "pipeline_model = pipeline.fit(train)\n",
    "predictions = pipeline_model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82a07697-99b0-4c97-ae3c-000e587864b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 10. Ngram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b60cea5d-d433-46d4-a219-df29140f3801",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram, VectorAssembler, StopWordsRemover, HashingTF, IDF, Tokenizer, StringIndexer, NGram, ChiSqSelector, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Use 90% cases for training, 10% cases for testing\n",
    "train, test = df_clean.randomSplit([0.9, 0.1], seed=20200819)\n",
    "\n",
    "# label\n",
    "label_encoder= StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "\n",
    "# Create transformers for the ML pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"1gram_idf\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"2gram\")\n",
    "ngram_hashingtf = HashingTF(inputCol=\"2gram\", outputCol=\"2gram_tf\", numFeatures=20000)\n",
    "ngram_idf = IDF(inputCol='2gram_tf', outputCol=\"2gram_idf\", minDocFreq=5) \n",
    "\n",
    "# Assemble all text features\n",
    "assembler = VectorAssembler(inputCols=[\"1gram_idf\", \"2gram_tf\"], outputCol=\"rawFeatures\")\n",
    "\n",
    "# Chi-square variable selection\n",
    "selector = ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")\n",
    "\n",
    "# Regression model estimator\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[label_encoder, tokenizer, stopword_remover, cv, idf, ngram, ngram_hashingtf, ngram_idf, assembler, selector, lr])\n",
    "\n",
    "# Pipeline model fitting\n",
    "pipeline_model = pipeline.fit(train)\n",
    "predictions = pipeline_model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1957055408484478,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "big_data_project",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
